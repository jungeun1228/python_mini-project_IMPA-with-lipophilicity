{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Set up an environment"
      ],
      "metadata": {
        "id": "zqaV6jEGTcCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel packaging\n",
        "#The session will automatically restart to use newly installed versions of packages"
      ],
      "metadata": {
        "id": "P6HURztFevni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG_xnoFsWFvr"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bring requirement file from google drive to content folder in colab local drive\n",
        "!pip install -q gdown\n",
        "!gdown --id 1OCq1jiHBm5kPrMO4AVVvgfuoXThDfJJJ -O /content/requirements-colab.txt\n",
        "!pip install -r requirements-colab.txt"
      ],
      "metadata": {
        "id": "DylIYowiEql8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clone IMPA github\n",
        "!git clone https://github.com/theislab/IMPA.git"
      ],
      "metadata": {
        "id": "ph_hBLKef0X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bring dataset tar file from google drive to content folder in colab local drive\n",
        "!pip install -q gdown\n",
        "!gdown --id 11Cu4nm64ZOaJyKMEORoehM2sJulGV5FP -O /content/bbbc021_all.tar.gz"
      ],
      "metadata": {
        "id": "06d0hGWcqEJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip the tar file with datasets in a newly created folder on colab drive\n",
        "!mkdir -p /content/IMPA/project_folder/datasets\n",
        "!tar -xvzf /content/bbbc021_all.tar.gz -C /content/IMPA/project_folder/datasets"
      ],
      "metadata": {
        "id": "sRFRgJJdGKXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bring checkpoint files from google drive to a created folder in colab local drive\n",
        "!mkdir -p /content/IMPA/checkpoints\n",
        "!gdown --folder 1W9zMjJYyRfdqYfhZLqi-V-wGER27nL5Z -O /content/IMPA/checkpoints/bbbc021_all"
      ],
      "metadata": {
        "id": "Thn-jtIxHkxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install IMPA package (setup.py)\n",
        "%cd /content/IMPA\n",
        "%pip install -e ."
      ],
      "metadata": {
        "id": "fUeoBbDxLMUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tutorial by Palma et al. (2024) was adjusted and the model was retrained through fine tuning\n",
        "\n",
        "original tutorial: \"Use IMPA for unseen drug prediction BBBC021\"\n",
        "\n",
        "https://github.com/theislab/IMPA/blob/main/tutorial/transform_cells_bbbc021_all_unseen_prediction.ipynb"
      ],
      "metadata": {
        "id": "YeSOLmORKJI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Import libraries\n",
        "# Standard library imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Local application/library imports\n",
        "from IMPA.dataset.data_loader import CellDataLoader\n",
        "from IMPA.solver import IMPAmodule\n",
        "import IMPA.eval.gan_metrics.fid as fid_mod\n",
        "from IMPA.eval.gan_metrics.density_and_coverage import compute_d_c\n",
        "\n",
        "# Third-party library imports\n",
        "import re\n",
        "import sys\n",
        "sys.path.append(\"/content/IMPA/tutorial\")\n",
        "from tutorial_utils import t2np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import linalg\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import inception_v3\n",
        "from omegaconf import OmegaConf\n",
        "from tqdm import tqdm\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "29o1sfKkKG8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Utils functions\n",
        "def transform_by_emb(solver, dataloader, y, n_average, args):\n",
        "    \"\"\"\n",
        "    Transform images in a dataloader using a solver for a specific drug ID.\n",
        "\n",
        "    Parameters:\n",
        "        solver: The solver object used for transformation.\n",
        "        dataloader: The dataloader containing images to be transformed.\n",
        "        n_average (int): Number of times to average random noise vectors.\n",
        "        drug_id (str): The ID of the drug for transformation.\n",
        "        args: Arguments object containing additional parameters.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two NumPy arrays representing controls and transformed images.\n",
        "    \"\"\"\n",
        "    controls = []\n",
        "    transformed = []\n",
        "\n",
        "    y = y.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader.train_dataloader()):\n",
        "            X_ctr = batch[\"X\"][0]\n",
        "            z = torch.ones(X_ctr.shape[0], n_average, args.z_dimension).cuda().mean(1)\n",
        "\n",
        "            # Perturbation ID\n",
        "            y_emb = y.repeat((z.shape[0], 1)).cuda()\n",
        "            y_emb = torch.cat([y_emb, z], dim=1)\n",
        "            y_emb = solver.nets.mapping_network(y_emb)\n",
        "\n",
        "            _, X_generated = solver.nets.generator(X_ctr, y_emb)\n",
        "            transformed.append(t2np(X_generated.detach().cpu(), batch_dim=True))\n",
        "            controls.append(t2np(X_ctr.detach().cpu(), batch_dim=True))\n",
        "            break\n",
        "    return np.concatenate(controls, axis=0), np.concatenate(transformed, axis=0)"
      ],
      "metadata": {
        "id": "f9uDDuR-KGzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Read the configuration of interest\n",
        "path_to_config = \"/content/IMPA/config_hydra/config/bbbc021_all.yaml\""
      ],
      "metadata": {
        "id": "Xyu9NwdjKGsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the YAML file\n",
        "with open(path_to_config, 'r') as file:\n",
        "    config = yaml.safe_load(file)  # Use safe_load to avoid executing arbitrary code\n",
        "\n",
        "# Correct the path to images and data index\n",
        "config[\"image_path\"] = \"IMPA/project_folder/datasets/bbbc021_all\"\n",
        "config[\"data_index_path\"] = \"IMPA/project_folder/datasets/bbbc021_all/metadata/bbbc021_df_all.csv\"\n",
        "config[\"embedding_path\"] = \"IMPA/embeddings/csv/emb_fp_all.csv\"\n",
        "\n",
        "with open(path_to_config, \"w\") as file:\n",
        "    yaml.safe_dump(config, file)\n",
        "\n",
        "# Access the loaded data\n",
        "print(config)"
      ],
      "metadata": {
        "id": "rvnr0S8pKKiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config[\"image_path\"] = \"/content/\" + config[\"image_path\"]\n",
        "config[\"data_index_path\"] = \"/content/\" + config[\"data_index_path\"]\n",
        "config[\"embedding_path\"] = \"/content/\" + config[\"embedding_path\"]"
      ],
      "metadata": {
        "id": "-jAe0KREKGS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Create an omega config dict\n",
        "args = OmegaConf.create(config)"
      ],
      "metadata": {
        "id": "BGl0GJeaM5aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Initialize data loader\n",
        "dataloader = CellDataLoader(args)\n",
        "train_dataloader = dataloader.train_dataloader()\n",
        "val_dataloader = dataloader.val_dataloader()"
      ],
      "metadata": {
        "id": "UB2RLgjoM5Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Wrapper for resizing images (96x96 --> 128x128: to use original checkpoint file)\n",
        "# imported images have 96x96 size --> 128x128 resize wrapper\n",
        "\n",
        "class ResizeWrapDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_ds, size=128):\n",
        "        self.base = base_ds\n",
        "        self.size = size\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "    def _resize_img(self, t):\n",
        "        if isinstance(t, torch.Tensor):\n",
        "            # t: [C,H,W]\n",
        "            if t.ndim == 3:\n",
        "                return F.interpolate(t.unsqueeze(0), size=(self.size, self.size),\n",
        "                                     mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "            # t: [H,W] (if grayscale)\n",
        "            if t.ndim == 2:\n",
        "                t = t.unsqueeze(0)  # [1,H,W]\n",
        "                return F.interpolate(t.unsqueeze(0), size=(self.size, self.size),\n",
        "                                     mode=\"bilinear\", align_corners=False).squeeze(0).squeeze(0)\n",
        "        return t\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.base[idx]\n",
        "        X = item[\"X\"]\n",
        "        # IMPA has (\"ctrl\",\"trt\") format - resize both\n",
        "        if isinstance(X, (list, tuple)) and len(X) == 2:\n",
        "            item[\"X\"] = (self._resize_img(X[0]), self._resize_img(X[1]))\n",
        "        else:\n",
        "            item[\"X\"] = self._resize_img(X)\n",
        "        return item\n",
        "\n",
        "# bring dataset, batch_size from old loader and create new loader\n",
        "train_base = train_dataloader.dataset\n",
        "val_base   = val_dataloader.dataset\n",
        "\n",
        "wrapped_train_loader = DataLoader(\n",
        "    ResizeWrapDataset(train_base, size=128),\n",
        "    batch_size=train_dataloader.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=getattr(args, \"num_workers\", 2),\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "wrapped_val_loader = DataLoader(\n",
        "    ResizeWrapDataset(val_base, size=128),\n",
        "    batch_size=val_dataloader.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=getattr(args, \"num_workers\", 2),\n",
        "    pin_memory=True,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "# check batch size\n",
        "b = next(iter(wrapped_train_loader))\n",
        "xc, xt = b[\"X\"]\n",
        "print(\"wrapped ctrl/trt shapes:\", tuple(xc.shape), tuple(xt.shape))  # -> (*, 3, 128, 128) 기대"
      ],
      "metadata": {
        "id": "DUL0zo4RM5By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Initialize model\n",
        "checkpoint_dir = \"/content/IMPA/checkpoints/bbbc021_all/\""
      ],
      "metadata": {
        "id": "XGFr1sZeNI0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Create model\n",
        "solver = IMPAmodule(args, checkpoint_dir, dataloader)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Oow8cFZSjsPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##avoid shape mismatch (domain 88→24): load checkpoint except discriminator head\n",
        "\n",
        "ckpt_path = \"/content/IMPA/checkpoints/bbbc021_all/checkpoint/000200_nets.ckpt\"\n",
        "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "\n",
        "def get_substate(ckpt_dict, keys):\n",
        "    for k in keys:\n",
        "        if k in ckpt_dict and isinstance(ckpt_dict[k], dict):\n",
        "            return ckpt_dict[k]\n",
        "    nets = ckpt_dict.get(\"nets\")\n",
        "    if isinstance(nets, dict):\n",
        "        for k in keys:\n",
        "            if k in nets and isinstance(nets[k], dict):\n",
        "                return nets[k]\n",
        "    return None\n",
        "\n",
        "def strip_module_prefix(sd):\n",
        "    return { (k[7:] if k.startswith(\"module.\") else k): v for k,v in sd.items() }\n",
        "\n",
        "def safe_load_module(module, state_dict, drop_head=False):\n",
        "    sd = strip_module_prefix(state_dict)\n",
        "    if drop_head:\n",
        "        sd = {k:v for k,v in sd.items() if not (k.startswith(\"head.\") or \".head.\" in k)}\n",
        "    target = module.module if hasattr(module, \"module\") else module\n",
        "    missing, unexpected = target.load_state_dict(sd, strict=False)\n",
        "    print(f\"[{target.__class__.__name__}] missing={len(missing)}, unexpected={len(unexpected)}\")\n",
        "    return missing, unexpected\n",
        "\n",
        "G_sd = get_substate(ckpt, [\"generator\"])\n",
        "D_sd = get_substate(ckpt, [\"discriminator\"])\n",
        "S_sd = get_substate(ckpt, [\"style_encoder\",\"styleencoder\"])\n",
        "M_sd = get_substate(ckpt, [\"mapping_network\",\"mappingNetwork\"])\n",
        "\n",
        "if G_sd is not None: safe_load_module(solver.generator, G_sd)\n",
        "if D_sd is not None: safe_load_module(solver.discriminator, D_sd, drop_head=True)  # ★ 핵심: head 제외\n",
        "if S_sd is not None: safe_load_module(solver.style_encoder, S_sd)\n",
        "if M_sd is not None: safe_load_module(solver.mapping_network, M_sd)"
      ],
      "metadata": {
        "id": "UusL10rkhhYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Linear probing: train only head\n",
        "def freeze_except_D_head(solver):\n",
        "    D = solver.discriminator.module if hasattr(solver.discriminator,\"module\") else solver.discriminator\n",
        "    # Discriminator: head only\n",
        "    for name, p in D.named_parameters():\n",
        "        p.requires_grad = name.startswith(\"module.head\") or name.startswith(\"head\")\n",
        "    # freeze rest of the modules (G, S, M)\n",
        "    for m in [solver.generator, solver.style_encoder, solver.mapping_network]:\n",
        "        mm = m.module if hasattr(m,\"module\") else m\n",
        "        for p in mm.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "# 2) Fine tuning\n",
        "def unfreeze_all(solver):\n",
        "    for m in [solver.discriminator, solver.generator, solver.style_encoder, solver.mapping_network]:\n",
        "        mm = m.module if hasattr(m,\"module\") else m\n",
        "        for p in mm.parameters():\n",
        "            p.requires_grad = True"
      ],
      "metadata": {
        "id": "wjn6InRspwSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training details\n",
        "# 1) Select mode — LP or FT\n",
        "mode = \"ft\"\n",
        "\n",
        "if mode == \"lp\":\n",
        "    freeze_except_D_head(solver)\n",
        "else:\n",
        "    unfreeze_all(solver)\n",
        "\n",
        "# fid tracking\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    monitor=\"fid_transformations\",  # based on yaml\n",
        "    mode=\"min\",\n",
        "    save_top_k=1,                   # only one best model\n",
        "    filename=\"{epoch:04d}-fid{fid_transformations:.2f}\",\n",
        ")\n",
        "\n",
        "# 2) Run training with PL Trainer (use original training_step/optimizers as-is)\n",
        "trainer = Trainer(\n",
        "    max_epochs=args.total_epochs,\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    devices=1 if torch.cuda.is_available() else None,\n",
        "    log_every_n_steps=getattr(args, \"log_every_n_steps\", 10),\n",
        "    callbacks=[checkpoint_cb],\n",
        ")\n",
        "\n",
        "### setting directories according to changed condition (wrapper)\n",
        "# 1) Prepare sample & checkpoint folders\n",
        "os.makedirs(os.path.join(checkpoint_dir, args.sample_dir), exist_ok=True)      # /content/IMPA/checkpoints/bbbc021_all/sample\n",
        "os.makedirs(os.path.join(checkpoint_dir, args.checkpoint_dir), exist_ok=True)  # /content/IMPA/checkpoints/bbbc021_all/checkpoint\n",
        "\n",
        "# 2) Replace validation loader used by solver for internal evaluation/visualization with 128x128 wrapping loader\n",
        "solver.loader_test = wrapped_val_loader"
      ],
      "metadata": {
        "id": "YhsBMf_iUbE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch FID sqrtm handling (to solve FID instability issue)\n",
        "def _sqrtm_robust(A):\n",
        "    out = linalg.sqrtm(A, disp=False)\n",
        "    if isinstance(out, tuple):\n",
        "        S, _ = out\n",
        "    else:\n",
        "        S = out\n",
        "    S = np.asarray(S)\n",
        "    # On numerical instability, add small jitter and recompute\n",
        "    if not np.isfinite(S).all():\n",
        "        jitter = np.eye(A.shape[0]) * 1e-6\n",
        "        out2 = linalg.sqrtm((A + jitter).astype(np.float64), disp=False)\n",
        "        S = out2[0] if isinstance(out2, tuple) else out2\n",
        "    return S.real\n",
        "\n",
        "def cal_frechet_distance_patched(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    mu1 = np.atleast_1d(mu1);   mu2 = np.atleast_1d(mu2)\n",
        "    sigma1 = np.atleast_2d(sigma1); sigma2 = np.atleast_2d(sigma2)\n",
        "    # Add small diagonal stabilization to covariance\n",
        "    epsI = np.eye(sigma1.shape[0]) * eps\n",
        "    covmean = _sqrtm_robust((sigma1 + epsI).dot(sigma2 + epsI))\n",
        "    diff = mu1 - mu2\n",
        "    fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2.0 * np.trace(covmean)\n",
        "    return float(np.real(fid))\n",
        "\n",
        "# Apply patch\n",
        "fid_mod.cal_frechet_distance = cal_frechet_distance_patched"
      ],
      "metadata": {
        "id": "skYk2Ns0BZAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training\n",
        "# Not a LightningDataModule → pass train/val dataloaders directly to fit\n",
        "# Use wrapped_*_loader here\n",
        "trainer.fit(\n",
        "    solver,\n",
        "    train_dataloaders=wrapped_train_loader,\n",
        "    val_dataloaders=wrapped_val_loader\n",
        ")"
      ],
      "metadata": {
        "id": "KoqW_3CRBdEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance metrics"
      ],
      "metadata": {
        "id": "4lfZfpqO2bVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print Best FID\n",
        "best_fid = float(checkpoint_cb.best_model_score)\n",
        "best_ckpt = checkpoint_cb.best_model_path\n",
        "print(\"Best FID:\", best_fid, \"| path:\", best_ckpt)"
      ],
      "metadata": {
        "id": "yVuQLaWtqWUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function: load checkpoint with the best FID\n",
        "#           and calculate coverage/precision/recall\n",
        "\n",
        "class InceptionPool3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        m = inception_v3(weights=None, aux_logits=False, transform_input=False)\n",
        "        self.features = nn.Sequential(*(list(m.children())[:-1]))  # up to avgpool\n",
        "        for p in self.features.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.eval()\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x):\n",
        "        if x.shape[-2:] != (299, 299):\n",
        "            x = F.interpolate(x, size=(299, 299), mode=\"bilinear\", align_corners=False)\n",
        "        x = torch.clamp(x, 0, 1)  # if [-1,1] -> (x+1)/2\n",
        "        f = self.features(x).view(x.size(0), -1)  # [B,2048]\n",
        "        return f.cpu().numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_coverage_prec_recall(solver, val_loader, device, k=None, max_batches=None):\n",
        "    feat_net = InceptionPool3().to(device)\n",
        "    real_feats, fake_feats = [], []\n",
        "\n",
        "    for i, batch in enumerate(val_loader):\n",
        "        X_ctrl, X_trt = batch[\"X\"]\n",
        "        y_trg = batch[\"mols\"].long().to(device)\n",
        "\n",
        "        X_ctrl = X_ctrl.to(device)\n",
        "        X_trt  = X_trt.to(device)\n",
        "\n",
        "        # Create target style (assuming single modality)\n",
        "        s_trg = solver.encode_label(X_ctrl, y_trg, None, None)\n",
        "        _, X_fake = solver.nets.generator(X_ctrl, s_trg)\n",
        "\n",
        "        # Uncomment below if conversion [-1,1] → [0,1] is needed\n",
        "        # X_trt  = torch.clamp((X_trt  + 1)/2, 0, 1)\n",
        "        # X_fake = torch.clamp((X_fake + 1)/2, 0, 1)\n",
        "\n",
        "        real_feats.append(feat_net(X_trt))\n",
        "        fake_feats.append(feat_net(X_fake))\n",
        "\n",
        "        if (max_batches is not None) and (i+1 >= max_batches):\n",
        "            break\n",
        "\n",
        "    real_feats = np.concatenate(real_feats, axis=0)\n",
        "    fake_feats = np.concatenate(fake_feats, axis=0)\n",
        "    N = min(len(real_feats), len(fake_feats))\n",
        "    if k is None:\n",
        "        k = max(3, int(np.sqrt(N)))\n",
        "\n",
        "    return compute_d_c(real_feats, fake_feats, nearest_k=k)\n"
      ],
      "metadata": {
        "id": "vX2bQhlq5rah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Compute P/R/D/C\n",
        "\n",
        "print(f\"Best FID: {best_fid:.4f} | path: {best_ckpt}\")\n",
        "\n",
        "# 1) Parse epoch number from Lightning ckpt → IMPA step = epoch + 1\n",
        "m = re.search(r\"epoch=(\\d+)\", os.path.basename(best_ckpt))\n",
        "assert m, f\"Cannot parse epoch from path: {best_ckpt}\"\n",
        "epoch_num = int(m.group(1))\n",
        "impa_step = epoch_num + 1  # IMPA saves at end of epoch with step = epoch + 1\n",
        "\n",
        "# 2) Optionally check for existing IMPA ckpt\n",
        "impa_nets = os.path.join(checkpoint_dir, args.checkpoint_dir, f\"{impa_step:06d}_nets.ckpt\")\n",
        "assert os.path.exists(impa_nets), f\"IMPA ckpt not found: {impa_nets}\"\n",
        "\n",
        "# 3) load best ckpt (IMPA format)\n",
        "solver._load_checkpoint(impa_step)\n",
        "\n",
        "# 4) Device alignment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "solver.to(device)\n",
        "for net in solver.nets.values():\n",
        "    (net.module if hasattr(net, \"module\") else net).to(device)\n",
        "\n",
        "em = solver.embedding_matrix\n",
        "if isinstance(em, (list, tuple)):\n",
        "    solver.embedding_matrix = type(em)([e.to(device) for e in em])\n",
        "else:\n",
        "    solver.embedding_matrix = em.to(device)\n",
        "\n",
        "# 5) Compute D/C/P/R\n",
        "dc = compute_coverage_prec_recall(\n",
        "    solver,\n",
        "    wrapped_val_loader,   # Recommend using 128x128 loader\n",
        "    device,\n",
        "    k=None,\n",
        "    max_batches=None\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"[Best-FID (Lightning epoch={epoch_num}) -> IMPA step={impa_step:06d}] \"\n",
        "    f\"precision: {dc['precision']:.4f} | \"\n",
        "    f\"recall: {dc['recall']:.4f} | \"\n",
        "    f\"density: {dc['density']:.4f} | \"\n",
        "    f\"coverage: {dc['coverage']:.4f}\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "DZPw9P-eG2u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application"
      ],
      "metadata": {
        "id": "FbdEFjgaWKTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####Tranform controls to perturbed\n",
        "# Initilize empty dictionaries\n",
        "controls = []\n",
        "transformed = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (drug, drug_id) in enumerate(dataloader.mol2id.items()):\n",
        "        print(f\"Transforming images for {drug}\")\n",
        "        transformed[drug] = []\n",
        "        for j, batch in tqdm(enumerate(dataloader.train_dataloader())):\n",
        "            X_ctr = batch[\"X\"][0]\n",
        "            # z original and z transported\n",
        "            z = torch.randn(X_ctr.shape[0], 100, args.z_dimension).cuda().mean(1)\n",
        "\n",
        "            # Perturbation ID\n",
        "            id_pert = dataloader.mol2id[drug] * torch.ones(X_ctr.shape[0]).long().cuda()\n",
        "            y = solver.embedding_matrix(id_pert)\n",
        "            y = torch.cat([y, z], dim=1)\n",
        "            y = solver.nets.mapping_network(y)\n",
        "\n",
        "            _, X_generated = solver.nets.generator(X_ctr, y)\n",
        "\n",
        "            if i==0:\n",
        "                controls.append(t2np(X_ctr.detach().cpu(), batch_dim=True))\n",
        "            transformed[drug].append(t2np(X_generated.detach().cpu(), batch_dim=True))\n",
        "            if j==3:\n",
        "                break\n",
        "\n",
        "controls = np.concatenate(controls, axis=0)\n",
        "transformed = {key: np.concatenate(val, axis=0) for key, val in transformed.items()}"
      ],
      "metadata": {
        "id": "PvWYZO6RNIgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(controls)):\n",
        "    print(f\"Control {i}\")\n",
        "    plt.figure(figsize=(1, 1))\n",
        "    plt.imshow(controls[i])\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    if i==3:\n",
        "        break"
      ],
      "metadata": {
        "id": "nfyxXj92NcXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pert in transformed:\n",
        "    print(f\"Perturbation {pert}\")\n",
        "    for i in range(len(transformed[pert])):\n",
        "        plt.figure(figsize=(1, 1))\n",
        "        plt.imshow(transformed[pert][i])\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "        if i==3:\n",
        "            break"
      ],
      "metadata": {
        "id": "29SKQBlrNcP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Predict on unseen perturbations\n",
        "ood_drugs = [\"taxol\",\n",
        "             \"ALLN\",\n",
        "             \"bryostatin\",\n",
        "             \"simvastatin\",\n",
        "             \"MG-132\",\n",
        "             \"methotrexate\",\n",
        "             \"colchicine\",\n",
        "             \"cytochalasin B\",\n",
        "             \"AZ258\",\n",
        "             \"cisplatin\"]"
      ],
      "metadata": {
        "id": "az8gT5Q_NnC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ood_drug_embeddings = pd.read_csv(\"/content/IMPA/embeddings/csv/emb_fp_all.csv\", index_col=0).loc[ood_drugs]"
      ],
      "metadata": {
        "id": "DLoAU2Z7Nq6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drugs = {}\n",
        "controls = {}\n",
        "\n",
        "for drug in ood_drugs:\n",
        "    print(f\"Transform into {drug}\")\n",
        "    emb_drug = torch.Tensor(ood_drug_embeddings.loc[drug])\n",
        "    control, transformed = transform_by_emb(solver, dataloader, emb_drug, 100, args)\n",
        "    drugs[drug] = transformed\n",
        "    controls[drug] = control"
      ],
      "metadata": {
        "id": "HlwgAx8LNt3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pert in controls:\n",
        "    print(f\"Perturbation {pert}\")\n",
        "    for i in range(len(drugs[pert])):\n",
        "        plt.figure(figsize=(1, 1))\n",
        "        plt.imshow(drugs[pert][i])\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "        if i==3:\n",
        "            break"
      ],
      "metadata": {
        "id": "oAc--PX5Ntu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References\n",
        "OpenAI. 2025. GPT-5, ChatGPT model. OpenAI, San Francisco, CA. Available at: https://chat.openai.com/ (accessed in August-September 2025).\n",
        "\n",
        "Palma A, Theis FJ, Lotfollahi M. 2025. Predicting cell morphological responses to perturbations using generative modeling. Nat Commun 16:505. DOI: 10.1038/s41467-024-55707-8.\n",
        "\n",
        "Palma A, Theis FJ, Lotfollahi M. 2025. IMPA: Predicting cell morphological responses to perturbations (GitHub repository). GitHub. Available at: https://github.com/theislab/impa (accessed in August-September, 2025)\n"
      ],
      "metadata": {
        "id": "0OB--aIHeIvP"
      }
    }
  ]
}